#!/usr/bin/env python3

"""Align words based on stepwise EM alignments with PMI scores."""

import collections
import itertools as it
import sys
import igraph, utils
import numpy as np
import random, codecs
import infomapcog.clustering as clust
import infomapcog.distances as distances

import argparse

import csv
import pickle

import lingpy
import infomapcog.ipa2asjp as ipa2asjp
from infomapcog.dataio import (read_data_cldf, read_data_lingpy,
                               read_data_ielex_type, OnlinePMITrainer)

import newick

readers = {
    "ielex": read_data_ielex_type,
    "cldf": read_data_cldf,
    "lingpy": read_data_lingpy,
    }

if __name__ == "__main__":

    # TODO:
    # - Add a ML based estimation of distance or a JC model for distance
    #   between two sequences
    # - Separate clustering code.
    # - Add doculect distance as regularization

    parser = argparse.ArgumentParser(
        description=__doc__)
    parser.add_argument(
        "--seed",
        type=int,
        default=1234,
        help="Random seed")
    parser.add_argument(
        "--max-iter",
        type=int,
        default=15,
        help="Maximum number of iterations")
    parser.add_argument(
        "--tolerance",
        type=float,
        default=0.001,
        help="TODO tolerance")
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.5,
        help="Threshold for the INFOMAP algorithm")
    parser.add_argument(
        "--max-batch",
        type=int,
        default=256,
        help="Maximum number of word pairs to align in one updating step")
    parser.add_argument(
        "--margin",
        type=float,
        default=1.0,
        help="TODO margin")
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.75,
        # Citation: Liang & Klein: Online EM for unsupervised models.
        help="""Stepsize reduction power α. Any 0.5 < α ≤ 1 is valid. The smaller
        the α, the larger the updates, and the more quickly old
        sufficient statistics decay. This can lead to swift progress
        but also generates instability.""")
    parser.add_argument(
        "--method",
        default='labelprop',
        choices=['infomap', 'labelprop', 'ebet', 'multilevel', 'spinglass'],
        help="Use this clustering method to find similar forms")
    parser.add_argument(
        "--gop",
        type=float,
        default=None,
        help="""Gap opening penalty in alignments (Default is to use
        character-dependent gap penalties.)""")
    parser.add_argument(
        "--gep",
        type=float,
        default=-1.75,
        help="""Gap extension penalty in alignments.""")
    parser.add_argument(
        "--guide-tree",
        type=argparse.FileType("r"),
        help="""A Newick file containing a single guide tree to combine multiple
        alignments. (Separate guide trees for different families are
        not supported yet.)""")
    parser.add_argument(
        "data",
        type=argparse.FileType("r"),
        help="IELex-style data file to read")
    parser.add_argument(
        "--transcription",
        default='ASJP',
        help="The transcription convention (IPA, ASJP, …) used in the data file")
    parser.add_argument(
        "--pmidict",
        type=argparse.FileType("wb"),
        help="Write PMI dictionary to this (pickle) file.")
    parser.add_argument(
        "--reader",
        choices=list(readers.keys()),
        default="ielex",
        help="Data file format")

    args = parser.parse_args()

    if args.alpha <= 0.5 or args.alpha > 1:
        raise ValueError("ALPHA must be in (0.5, 1].")

    random.seed(args.seed)
    tolerance = args.tolerance
    alpha = args.alpha

    data_dict, cogid_dict, words_dict, langs_list, char_list = (
        readers[args.reader](args.data, data=args.transcription))
    print("Character set:", char_list, "({:d})".format(len(char_list)))

    word_pairs = []
    for concept in data_dict:
        words = data_dict[concept].items()
        for ((i1, l1), w1), ((i2, l2), w2) in it.combinations(words, r=2):
            if distances.normalized_leventsthein(w1, w2) <= 0.5:
                word_pairs.append(((concept, l1, w1), (concept, l2, w2)))
    print()

    print("Size of initial list ", len(word_pairs))

    online = OnlinePMITrainer(
        alpha=args.alpha,
        margin=args.margin)

    # We might miss some cognate pairs because they look dissimilar,
    # but they are actually connected strongly when taking regular
    # sound correspondences into account. So we run the iterator once
    # on the reduced word list to obtain an initial PMI score
    # matrix. Then we split the word list again into chunks that are
    # similar according to *that* scorer.

    print("Calculating PMIs from very similar words.")
    for n_iter in range(0, args.max_iter):
        random.shuffle(word_pairs)
        print("Iteration", n_iter)
        idx = 0
        n_zero = 0
        while idx < len(word_pairs):
            wl = word_pairs[idx:idx+args.max_batch]
            algn_list, z = online.align_pairs(wl)
            n_zero += z
            word_pairs[idx:idx+args.max_batch] = wl
            idx += len(wl)

        print("Non zero examples went down to {:d} (-{:d}). Updates: {:d}".format(
            len(word_pairs), n_zero, online.n_updates))
        print(collections.Counter(online.pmidict).most_common(8)[::2])

    if args.pmidict:
        print(collections.Counter(online.pmidict).most_common()[::2])
        pickle.dump(online.pmidict, args.pmidict)

    codes = clust.cognate_code_infomap2(
        words_dict, online.pmidict,
        gop=args.gop, gep=args.gep, threshold=args.threshold, method=args.method)
    for similarityset in codes:
        print("=================")
        for entry in similarityset:
            print(entry)
