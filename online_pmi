#!/usr/bin/env python3

from collections import defaultdict
import itertools as it
import sys
import igraph, utils
import numpy as np
import random, codecs
import infomapcog.clustering as clust
import infomapcog.distances as distances
from infomapcog.ipa2asjp import ipa2asjp

import argparse

import csv
import pickle


def clean_word(w):
    w = w.replace("-","")
    w = w.replace(" ", "")
    w = w.replace("%","")
    w = w.replace("~","")
    w = w.replace("*","")
    w = w.replace("$","")
    w = w.replace("\"","")
    w = w.replace("K","k")
    w = w.replace("|","")
    w = w.replace(".","")
    w = w.replace("+","")
    w = w.replace("·","")
    w = w.replace("?","")
    w = w.replace("’","")
    w = w.replace("]","")
    w = w.replace("[","")
    w = w.replace("=","")
    w = w.replace("_","")
    w = w.replace("<","")
    w = w.replace(">","")
    w = w.replace("‐","")
    w = w.replace("ᶢ","")
    w = w.replace("C","c")
    w = w.replace("L","l")
    w = w.replace("W","w")
    w = w.replace("T","t")
    return w


def ipa2sca(w):
    return "".join(tokens2class(ipa2tokens(w), 'asjp')).replace("0","")


def read_data_ielex_type(datafile, char_list=set()):
    """Read an IELex style TSV file."""
    line_id = 0
    data_dict = defaultdict(lambda : defaultdict())
    cogid_dict = defaultdict(lambda : defaultdict())
    words_dict = defaultdict(lambda : defaultdict(list))
    langs_list = []
    
    datafile.readline()
    for line in datafile:
        line = line.strip()
        arr = line.split("\t")
        lang = arr[0]

        concept = arr[2]
        cogid = arr[6]
        cogid = cogid.replace("-","")
        cogid = cogid.replace("?","")
        asjp_word = clean_word(arr[5].split(",")[0])

        for ch in asjp_word:
            if ch not in char_list:
                char_list.add(ch)

        if len(asjp_word) < 1:
            continue

        data_dict[concept][line_id,lang] = asjp_word
        cogid_dict[concept][line_id,lang] = cogid
        words_dict[concept][lang].append(asjp_word)
        if lang not in langs_list:
            langs_list.append(lang)
        line_id += 1
    
    return (data_dict, cogid_dict, words_dict, langs_list, char_list)


def read_data_cldf(datafile, sep="\t", char_list=set()):
    """Read a CLDF file in TSV or CSV format."""
    reader = csv.DictReader(
        datafile,
        dialect='excel' if sep == ',' else 'excel-tab')
    langs = set()
    data_dict = defaultdict(lambda : defaultdict())
    cogid_dict = defaultdict(lambda : defaultdict())
    words_dict = defaultdict(lambda : defaultdict(list))
    for line, row in enumerate(reader):
        lang = row["Language ID"]
        langs.add(lang)

        try:
            asjp_word = clean_word(row["ASJP"])
        except KeyError:
            asjp_word = ipa2asjp(row["IPA"])
            
        if not asjp_word:
            continue

        for ch in asjp_word:
            if ch not in char_list:
                char_list.add(ch)

        concept = row["Feature ID"]
        cogid = row["Cognate Class"]

        data_dict[concept][line, lang] = asjp_word
        cogid_dict[concept][line, lang] = cogid
        words_dict[concept].setdefault(lang, []).append(asjp_word)

    return (data_dict, cogid_dict, words_dict, list(langs), char_list)


def read_data_lingpy(datafile, sep="\t", char_list=set()):
    """Read a Lingpy file in TSV or CSV format."""
    reader = csv.DictReader(
        datafile,
        dialect='excel' if sep == ',' else 'excel-tab')
    langs = set()
    data_dict = defaultdict(lambda : defaultdict())
    cogid_dict = defaultdict(lambda : defaultdict())
    words_dict = defaultdict(lambda : defaultdict(list))
    for line, row in enumerate(reader):
        lang = row["DOCULECT"]
        langs.add(lang)

        try:
            asjp_word = clean_word(row["ASJP"])
        except KeyError:
            asjp_word = ipa2asjp(row["IPA"])
        if not asjp_word:
            continue

        for ch in asjp_word:
            if ch not in char_list:
                char_list.add(ch)

        concept = row["CONCEPT"]
        cogid = row["COGID"]

        data_dict[concept][line, lang] = asjp_word
        cogid_dict[concept][line, lang] = cogid
        words_dict[concept].setdefault(lang, []).append(asjp_word)

    return (data_dict, cogid_dict, words_dict, list(langs), char_list)


def calc_pmi(alignment_dict, char_list, scores, initialize=False):
    sound_dict = defaultdict(float)
    relative_align_freq = 0.0
    relative_sound_freq = 0.0
    count_dict = defaultdict(float)
    
    if initialize == True:
        for c1, c2 in it.product(char_list, repeat=2):
            if c1 == "" or c2 == "":
                continue
            count_dict[c1,c2] += 0.001
            count_dict[c2,c1] += 0.001
            sound_dict[c1] += 0.001
            sound_dict[c2] += 0.001
            relative_align_freq += 0.001
            relative_sound_freq += 0.002

    for alignment, score in zip(alignment_dict, scores):
        score = 1.0
        for a1, a2 in alignment:
            if a1 == "" or a2 == "":
                continue
            count_dict[a1,a2] += 1.0*score
            count_dict[a2,a1] += 1.0*score
            sound_dict[a1] += 2.0*score
            sound_dict[a2] += 2.0*score
            #relative_align_freq += 2.0
            #relative_sound_freq += 2.0

    relative_align_freq = sum(list(count_dict.values()))
    relative_sound_freq = sum(list(sound_dict.values()))
    
    for a in count_dict.keys():
        m = count_dict[a]
        assert m>0

        num = np.log(m)-np.log(relative_align_freq)
        denom = np.log(sound_dict[a[0]])+np.log(sound_dict[a[1]])-(2.0*np.log(relative_sound_freq))
        val = num - denom
        count_dict[a] = val
    
    return count_dict


readers = {
    "ielex": read_data_ielex_type,
    "cldf": read_data_cldf,
    "lingpy": read_data_lingpy,
    }

if __name__ == "__main__":

    # TODO:
    # - Add a ML based estimation of distance or a JC model for distance
    #   between two sequences
    # - Separate clustering code.
    # - Add doculect distance as regularization

    parser = argparse.ArgumentParser(
        description="Calculate pmi scores, or something.")
    parser.add_argument(
        "--seed",
        type=int,
        default=1234,
        help="Random seed")
    parser.add_argument(
        "--max-iter",
        type=int,
        default=15,
        help="Maximum number of iterations")
    parser.add_argument(
        "--tolerance",
        type=float,
        default=0.001,
        help="TODO tolerance")
    parser.add_argument(
        "--infomap-threshold",
        type=float,
        default=0.5,
        help="Threshold for the INFOMAP algorithm")
    parser.add_argument(
        "--min-batch",
        type=int,
        default=256,
        help="TODO min batch")
    parser.add_argument(
        "--margin",
        type=float,
        default=1.0,
        help="TODO margin")
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.75,
        help="TODO alpha")
    parser.add_argument(
        "data",
        type=argparse.FileType("r"),
        help="IELex-style data file to read")
    parser.add_argument(
        "--pmidict",
        type=argparse.FileType("wb"),
        help="Write PMI dictionary to this (pickle) file.")
    parser.add_argument(
        "--reader",
        choices=list(readers.keys()),
        default="ielex",
        help="Data file format")
    
    args = parser.parse_args()

    random.seed(args.seed)
    MAX_ITER = args.max_iter
    tolerance = args.tolerance
    infomap_threshold = args.infomap_threshold
    min_batch = args.min_batch
    margin = args.margin
    alpha = args.alpha
    data = args.data

    data_dict, cogid_dict, words_dict, langs_list, char_list = (
        readers[args.reader](data))
    print("Character list:", char_list, "({:d})".format(len(char_list)))

    word_pairs = []

    for concept in data_dict:
        print(concept, end=", ")
        words = data_dict[concept].values()
        for x, y in it.combinations(words, r=2):
            if distances.normalized_leventsthein(x, y) <= 0.5:
                word_pairs.append((x, y))
    print()

    print("Size of initial list ", len(word_pairs))

    pmidict = {}
    n_updates = 0
    for n_iter in range(MAX_ITER):
        random.shuffle(word_pairs)
        pruned_wl = []
        n_zero = 0
        print("Iteration ", n_iter)
        algn_list, scores = [], []
        for idx in range(0, len(word_pairs), min_batch):
            mb_pmi_dict = calc_pmi(algn_list, char_list, scores)
            eta = (n_updates + 2) ** (-alpha)
            for k, v in mb_pmi_dict.items():
                pmidict_val = pmidict.get(k, 0.0)
                pmidict[k] = (eta * v) + ((1.0 - eta) * pmidict_val)
            wl = word_pairs[idx:idx+min_batch]
            algn_list, scores = [], []
            n_updates += 1
            for w1, w2 in wl:
                s, alg = distances.needleman_wunsch(
                    w1, w2, gop=-2.5, gep=-1.75, lodict=pmidict)
                if s <= margin:
                    n_zero += 1
                    continue
                algn_list.append(alg)
                scores.append(s)
                pruned_wl.append([w1, w2])
            for x in algn_list:
                print(x)

        print("Non zero examples ", len(word_pairs), len(word_pairs)-n_zero,
              " number of updates ", n_updates)
        print(pmidict)
        word_pairs = pruned_wl

    if args.pmidict:
        pickle.dump(pmidict,
                    args.pmidict)
    clust.infomap_concept_evaluate_scores(
        data_dict, pmidict, -2.5, -1.75, infomap_threshold, cogid_dict)
